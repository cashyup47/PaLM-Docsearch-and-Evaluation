{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PDF in question must be parseable (meaning that you should be able to highlight it when on a PDF editor)\n",
    "\n",
    "This program trains itself with information from any PDF, takes an input question, finds the most appropriate reference text (page) and then gives the response. This should be run before the criteria evaluation and embedding distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the OS with your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "#insert your own Google API key inside the quotations\n",
    "palm.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the appropriate model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [m for m in palm.list_models() if 'embedText' in m.supported_generation_methods]\n",
    "model1 = models[0].name\n",
    "#print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and splitting the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"\")\n",
    "# Put your PDF file name in the above quotations. Ensure that this file and the PDF are in the same folder.\n",
    "pages = pdf_loader.load_and_split()\n",
    "\n",
    "# This step may take longer depending on the PDF length.\n",
    "\n",
    "content = []\n",
    "for page in pages:\n",
    "    text_content = page.page_content\n",
    "    content.append(text_content)\n",
    "\n",
    "# BELOW IS JUST SAMPLE CODE TO TEST IF THE EMBEDDINGS WORK\n",
    "# sample_text = content[7]\n",
    "# print(sample_text)\n",
    "# embedding = palm.generate_embeddings(model=model1, text=sample_text)\n",
    "# print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the content into a pandas dataframe so that you can add extra columns to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(content)\n",
    "df.columns = [\"Text\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the embeddings to the Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_fn(text):\n",
    "  return palm.generate_embeddings(model=model1, text=text)['embedding']\n",
    "\n",
    "df['Embeddings'] = df['Text'].apply(embed_fn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User input to enter the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Enter a question: \")\n",
    "# the input is at the top of this code interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for finding the best passage with the question as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_passage(query, dataframe):\n",
    "  \"\"\"\n",
    "  Compute the distances between the query and each document in the dataframe\n",
    "  using the dot product.\n",
    "  \"\"\"\n",
    "  query_embedding = palm.generate_embeddings(model=model1, text=query)\n",
    "  dot_products = np.dot(np.stack(dataframe['Embeddings']), query_embedding['embedding'])\n",
    "  idx = np.argmax(dot_products)\n",
    "  return dataframe.iloc[idx]['Text'] # Return text from index with max value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the appropriate passage and printing it out (can comment out the second line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_passage = find_best_passage(question, df)\n",
    "print(best_passage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to make an appropriate prompt that AI can understand based on the passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(query, relevant_passage):\n",
    "  escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\")\n",
    "  prompt = textwrap.dedent(\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
    "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
    "  However, you are talking to a non-technical audience, so be sure to break down complicated concepts and strike a friendly and converstional tone.\n",
    "  If the passage is irrelevant to the answer, you may ignore it.\n",
    "  QUESTION: '{query}'\n",
    "  \n",
    "  PASSAGE: \n",
    "  {relevant_passage}\n",
    "\n",
    "  \n",
    "    ANSWER:\n",
    "  \"\"\").format(query=query, relevant_passage=escaped)\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to generate the answer based on the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt):\n",
    "    text_models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "    text_model = text_models[0]\n",
    "    temperature = 0\n",
    "    # feel free to experiment with the temperature \n",
    "    # (0 means it is the same response each time, 1 means that the responses are random)\n",
    "\n",
    "    answer = palm.generate_text(prompt=prompt,\n",
    "                            model=text_model,\n",
    "                            candidate_count=1,\n",
    "                            temperature=temperature,\n",
    "                            max_output_tokens=1000)\n",
    "    # feel free to experiment with the candidate count (the number of answers it gives)\n",
    "    \n",
    "    for i, candidate in enumerate(answer.candidates):\n",
    "        print(f\"Candidate {i}: {candidate['output']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the methods to generate the prompt and answer to it and printing both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = make_prompt(question, best_passage)\n",
    "print(prompt)\n",
    "generate_answer(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
