{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimers: \n",
    "This code implies that everything is used in an Excel spreadsheet.\n",
    "The main notebook should have been run first to extract a response.\n",
    "The AI response, input question and reference text are all stored in a spreadsheet.\n",
    "\n",
    "If you did not use a spreadsheet, manually paste the appropriate responses in the last code segment.\n",
    "A response of 1 means that the prediction is \"correct\"; it is in line with the reference text.\n",
    "A response of 0 means that the prediction is \"incorrect\"; it is not in line with the reference text;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "import google.generativeai as palm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the OS environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "#insert your own Google API key inside the quotations\n",
    "palm.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Google PaLM and setting up the evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "\n",
    "llm = GooglePalm(temperature=0)\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", llm=llm, labeled_criteria=\"correctness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to import cells and text from a Microsoft Excel spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the name of your spreadsheet inside the first set of quotations\n",
    "# write the columns you will need in the second set of quotations\n",
    "# Example of columns: if you need to use columns C, D, and H, write 'C, D, H'\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_excel(\"\", usecols = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will automatically \n",
    "\n",
    "Disclaimer:\n",
    "The following code assumes that the question is in the first column, the reference text in the second column, and the AI response in the third. Modify this based on what columns you have your specific text in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for row in range(23, 28):\n",
    "    eval_result = evaluator.evaluate_strings(\n",
    "        prediction=df.iloc[row,2],\n",
    "        input=df.iloc[row,0],\n",
    "        reference=df.iloc[row,1]\n",
    "    )\n",
    "    print(eval_result[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not use a spreadsheet, you can manually paste your AI response in \"prediction\", question in \"input\" and source text/best passage in \"reference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kashy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\evaluation\\schema.py:108: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
      "To use references, use the labeled_criteria instead.\n",
      "  warn(self._skip_reference_warning)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m eval_result \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39;49mevaluate_strings(\n",
      "\u001b[0;32m      2\u001b[0m     prediction\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mThis model handbook is intended to help employers prepare their own employee handbooks. It contains a variety of policies and procedures that employers may want to include in their handbooks, such as an employment at-will disclaimer, a statement regarding equal employment opportunity, a policy prohibiting unlawful discrimination and harassment, a section that describes the policy for use of company property and privacy rules, a section on employment classification and overtime rules, a policy on Family and Medical Leave if you have 50 or more employees, and a section on Safety.Â \u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mWhy was this handbook written?Â \u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m      4\u001b[0m     reference\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTo inform and establish company policies and expectations.\u001b[39;49m\u001b[39m\"\u001b[39;49m,   \n",
      "\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(eval_result[\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kashy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\evaluation\\schema.py:195\u001b[0m, in \u001b[0;36mStringEvaluator.evaluate_strings\u001b[1;34m(self, prediction, reference, input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    184\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Evaluate Chain or LLM output, based on optional input and label.\u001b[39;00m\n",
      "\u001b[0;32m    185\u001b[0m \n",
      "\u001b[0;32m    186\u001b[0m \u001b[39mArgs:\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    192\u001b[0m \u001b[39m    dict: The evaluation results containing the score or value.\u001b[39;00m\n",
      "\u001b[0;32m    193\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n",
      "\u001b[0;32m    194\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_evaluation_args(reference\u001b[39m=\u001b[39mreference, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m)\n",
      "\u001b[1;32m--> 195\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_strings(\n",
      "\u001b[0;32m    196\u001b[0m     prediction\u001b[39m=\u001b[39;49mprediction, reference\u001b[39m=\u001b[39;49mreference, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n",
      "\u001b[0;32m    197\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kashy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py:419\u001b[0m, in \u001b[0;36mCriteriaEvalChain._evaluate_strings\u001b[1;34m(self, prediction, reference, input, callbacks, tags, metadata, include_run_info, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    385\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Evaluate a prediction against the criteria.\u001b[39;00m\n",
      "\u001b[0;32m    386\u001b[0m \n",
      "\u001b[0;32m    387\u001b[0m \u001b[39mParameters\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    416\u001b[0m \u001b[39m    )\u001b[39;00m\n",
      "\u001b[0;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    418\u001b[0m input_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_eval_input(prediction, reference, \u001b[39minput\u001b[39m)\n",
      "\u001b[1;32m--> 419\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n",
      "\u001b[0;32m    420\u001b[0m     input_,\n",
      "\u001b[0;32m    421\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n",
      "\u001b[0;32m    422\u001b[0m     tags\u001b[39m=\u001b[39;49mtags,\n",
      "\u001b[0;32m    423\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n",
      "\u001b[0;32m    424\u001b[0m     include_run_info\u001b[39m=\u001b[39;49minclude_run_info,\n",
      "\u001b[0;32m    425\u001b[0m )\n",
      "\u001b[0;32m    426\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_output(result)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kashy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:258\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m    257\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "\u001b[1;32m--> 258\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[0;32m    259\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n",
      "\u001b[0;32m    260\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n",
      "\u001b[0;32m    261\u001b[0m     inputs, outputs, return_only_outputs\n",
      "\u001b[0;32m    262\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kashy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:252\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n",
      "\u001b[0;32m    246\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n",
      "\u001b[0;32m    247\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n",
      "\u001b[0;32m    248\u001b[0m     inputs,\n",
      "\u001b[0;32m    249\u001b[0m )\n",
      "\u001b[0;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m    251\u001b[0m     outputs \u001b[39m=\u001b[39m (\n",
      "\u001b[1;32m--> 252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n",
      "\u001b[0;32m    253\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n",
      "\u001b[0;32m    254\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n",
      "\u001b[0;32m    255\u001b[0m     )\n",
      "\u001b[0;32m    256\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m    257\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kashy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:93\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n",
      "\u001b[0;32m     88\u001b[0m     \u001b[39mself\u001b[39m,\n",
      "\u001b[0;32m     89\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n",
      "\u001b[0;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n",
      "\u001b[0;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n",
      "\u001b[0;32m     92\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate([inputs], run_manager\u001b[39m=\u001b[39mrun_manager)\n",
      "\u001b[1;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kashy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:217\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[1;34m(self, llm_result)\u001b[0m\n",
      "\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_outputs\u001b[39m(\u001b[39mself\u001b[39m, llm_result: LLMResult) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, Any]]:\n",
      "\u001b[0;32m    216\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 217\u001b[0m     result \u001b[39m=\u001b[39m [\n",
      "\u001b[0;32m    218\u001b[0m         \u001b[39m# Get the text of the top generated string.\u001b[39;49;00m\n",
      "\u001b[0;32m    219\u001b[0m         {\n",
      "\u001b[0;32m    220\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_key: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse_result(generation),\n",
      "\u001b[0;32m    221\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfull_generation\u001b[39;49m\u001b[39m\"\u001b[39;49m: generation,\n",
      "\u001b[0;32m    222\u001b[0m         }\n",
      "\u001b[0;32m    223\u001b[0m         \u001b[39mfor\u001b[39;49;00m generation \u001b[39min\u001b[39;49;00m llm_result\u001b[39m.\u001b[39;49mgenerations\n",
      "\u001b[0;32m    224\u001b[0m     ]\n",
      "\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_final_only:\n",
      "\u001b[0;32m    226\u001b[0m         result \u001b[39m=\u001b[39m [{\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: r[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]} \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m result]\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kashy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:220\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_outputs\u001b[39m(\u001b[39mself\u001b[39m, llm_result: LLMResult) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, Any]]:\n",
      "\u001b[0;32m    216\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    217\u001b[0m     result \u001b[39m=\u001b[39m [\n",
      "\u001b[0;32m    218\u001b[0m         \u001b[39m# Get the text of the top generated string.\u001b[39;00m\n",
      "\u001b[0;32m    219\u001b[0m         {\n",
      "\u001b[1;32m--> 220\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse_result(generation),\n",
      "\u001b[0;32m    221\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfull_generation\u001b[39m\u001b[39m\"\u001b[39m: generation,\n",
      "\u001b[0;32m    222\u001b[0m         }\n",
      "\u001b[0;32m    223\u001b[0m         \u001b[39mfor\u001b[39;00m generation \u001b[39min\u001b[39;00m llm_result\u001b[39m.\u001b[39mgenerations\n",
      "\u001b[0;32m    224\u001b[0m     ]\n",
      "\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_final_only:\n",
      "\u001b[0;32m    226\u001b[0m         result \u001b[39m=\u001b[39m [{\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: r[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]} \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m result]\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kashy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\schema\\output_parser.py:115\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[1;34m(self, result)\u001b[0m\n",
      "\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_result\u001b[39m(\u001b[39mself\u001b[39m, result: List[Generation]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n",
      "\u001b[0;32m    103\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n",
      "\u001b[0;32m    104\u001b[0m \n",
      "\u001b[0;32m    105\u001b[0m \u001b[39m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    113\u001b[0m \u001b[39m        Structured output.\u001b[39;00m\n",
      "\u001b[0;32m    114\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse(result[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mtext)\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"This model handbook is intended to help employers prepare their own employee handbooks. It contains a variety of policies and procedures that employers may want to include in their handbooks, such as an employment at-will disclaimer, a statement regarding equal employment opportunity, a policy prohibiting unlawful discrimination and harassment, a section that describes the policy for use of company property and privacy rules, a section on employment classification and overtime rules, a policy on Family and Medical Leave if you have 50 or more employees, and a section on Safety.Â \",\n",
    "    input=\"Why was this handbook written?Â \",\n",
    "    reference=\"To inform and establish company policies and expectations.\",   \n",
    ")\n",
    "print(eval_result[\"score\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
